\input texinfo

@c % /*************************************************************************
@c %  *                              PART I: HEADER                           *
@c %  *************************************************************************/
@setfilename polyopt.info
@settitle PolyOpt - Polyhedral Optimization Framework

@set EDITION 0.2
@set VERSION 0.2.1
@set UPDATED March 12th 2012
@setchapternewpage odd


@c % /*************************************************************************
@c %  *                 PART II: SUMMARY DESCRIPTION AND COPYRIGHT            *
@c %  *************************************************************************/

@copying
This manual is dedicated to PolyOpt/C version @value{VERSION}, a
framework for Polyhedral Optimization in the ROSE compiler. 

Copyright @copyright{} 2009-2011 Louis-No@"el Pouchet / the Ohio State University.

@c quotation
Permission is granted to copy, distribute and/or modify this document under
the terms of the GNU Free Documentation License, Version 1.2
published by the Free Software Foundation. To receive a copy of the
GNU Free Documentation License, write to the Free Software Foundation, Inc.,
59 Temple Place, Suite 330, Boston, MA  02111-1307 USA.
@c end quotation
@end copying

@c % /*************************************************************************
@c %  *                 PART III: TITLEPAGE, CONTENTS, COPYRIGHT              *
@c %  *************************************************************************/
@titlepage
@title PolyOpt/C
@subtitle A Polyhedral Optimizer for the ROSE compiler
@subtitle Edition @value{EDITION}, for PolyOpt/C @value{VERSION}
@subtitle @value{UPDATED}
@author Louis-No@"el Pouchet

@c The following two commands start the copyright page.
@page

@vskip 0pt plus 1filll
@insertcopying
@end titlepage

@c Output the table of contents at the beginning.
@contents

@c % /*************************************************************************
@c %  *                     PART IV: TOP NODE AND MASTER MENU                 *
@c %  *************************************************************************/
@ifnottex
@node Top
@top PolyOpt/C

@insertcopying
@end ifnottex



@menu
* Introduction::
* Specifics of Polyhedral Programs::
* Optimization Paths::
* Fine-tuning Optimizations::
* Troubleshooting::
* References::
@end menu



@c % /*************************************************************************
@c %  *                       PART V: BODY OF THE DOCUMENT                    *
@c %  *************************************************************************/

@node Introduction
@chapter Introduction


PolyOpt is a polyhedral loop optimization framework, integrated in the ROSE compiler. The main features are:
@itemize
@item Automatic extraction of regions that can be optimized in the polyhedral model
@item Full support of PoCC (the Polyhedral Compiler Collection) analysis and optimizations
@itemize
@item Dependence analysis with Candl
@item Program transformations for tiling and parallelism with Pluto
@item Code generation with CLooG
@item Parametric tiling with PTile
@item Numerous under-the-hood functionalities and optimizations
@end itemize
@end itemize

Note, only a subset of C/C++ is currently supported by PolyOpt/C. See PolyOpt/Fortran for Fortran support.


@*
@b{Communication}:
The main mailing list is @uref{mailto:polyhedral@@cse.ohio-state.edu,polyhedral@@cse.ohio-state.edu}. You can also contact  directly Louis-Noel Pouchet @uref{mailto:pouchet@@cse.ohio-state.edu,pouchet@@cse.ohio-state.edu} for any question. PoCC is also  available as a stand-alone software on @uref{http://pocc.sourceforge.net, sourceforge}
@*

@c % /*************************************************************************
@c % /*************************************************************************

@node Specifics of Polyhedral Programs
@chapter Specifics of Polyhedral Programs

@section Static Control Parts

@* Sequences of (possibly imperfectly nested) loops amenable to
polyhedral optimization are called @emph{static control parts} (SCoP)
[5], roughly defined as a set of consecutive statements such that all
loop bounds and conditionals are affine functions of the surrounding
loop iterators and parameters (variables whose value is unknown at
compile time but invariant in the loop nest considered). In addition,
for effective data dependence analysis we require the array access
functions to also be affine functions of the surrounding iterators and
parameters.

For instance, a valid affine expression for a conditional or a loop bound in a SCoP with three loops iterators @emph{i,j,k} and two parameters @emph{N,P}  will be
of the form @emph{a.i + b.j + c.k + d.N + e.P + f},  @emph{a,b,c,d,e,f} are arbitrary (possibly 0) integer numbers.

The following program is a SCoP:
@*

@cartouche
@example
    for (i = 0; i < N; i++)
        for (j = 0; j < N; j++) @{
            A[i][j] = A[i][j] + u1[i]*v1[j]
            if (N - i > 2)
               A[i][j] -= 2;
         @}
@end example
@end cartouche

@*
Numerous elements can break the SCoP property, for instance:
@itemize
@item @code{if} conditionals involving variables that are not a loop iterator or a parameter, e.g., @code{if (A[i][j] == 0)}.
@item @code{if} conditionals involving loop iterators and/or a parameter to form a non-affine expression, e.g., @code{if (i * j == 0)}.
@item Non-affine @code{for} initialization or test condition, e.g., @code{for (j = 0; j < i * i; ++i)}.
@item Non-affine array access, e.g., @code{A[i*N][j % i]} or @code{A[B[i]]}.
@end itemize
@*

@section Additional Restrictions in PolyOpt/C

@*

PolyOpt/C automatically extracts maximal regions that can be optimized
in the Polyhedral framework. We enforce numerous additional
constraints to ensure the correctness of the SCoP extraction, in
particular due to dependence analysis consideration:
@itemize
@item The only allowed control statements are @code{for} and @code{if}.
@item There is no function call in the SCoP.
@item There is no explicit pointer arithmetic/manipulation in the SCoP (no @code{&} nor @code{*} operators).
@item @code{goto}, @code{break} and @code{continue} statements are forbidden.
@item Arrays represent distinct memory locations (one per accessed array cell), and arrays are not aliased (note: @b{no check is performed by PolyOpt/C for this property, it is the responsibility of the user to not feed ill-formed arrays}).
@item Loops increment by step of one.
@end itemize

@*


@section Allowed Control-Flow Operations

@*

PolyOpt/C supports a wide range of affine expressions, in particular
conjunctions of affine constraints can be used to bound the space. In
all the following, we recall that an affine expression must involve
only surrounding loop iterators and parameters (scalar variables that
are invariant during the SCoP execution).

SCoP extraction is a syntactic process so there are clear definitions
of what is allowed in @code{for (init; test; increment)} and @code{if
(condition)} statements. We note that if the loop iterator of a
@code{for} statement is used outside the scope of the loop, or is
assigned in the loop body, the @emph{loop will conservatively not be
considered for SCoP extraction} since PolyOpt/C may change the exit
value of loop iterators.

@*

@subsection In @code{for} initialization statement

@*
@code{init} can be either empty, or of the from @code{<type> var =
expressionLb}. That is, a single variable initialization is
supported. The @code{expressionLb} is an affine expression, or
possibly a conjunction of expressions with the @code{max(expr1,
expr2)} operator. The @code{max} operator can either be written using
a call to the @code{max} function together with using the
@code{--polyopt-safe-math-func} flag, or with the ternary operation @code{a
< b ? b : a}.

If the loop has no lower bound, the polyhedral representation will
assume an infinite lower bound for the loop iterator: no analysis is
performed to determine if there exists an initialization of the loop
iterator before the @code{for} statement.

As an illustration, all loops in the following code form
a valid SCoP.

@cartouche
@example
    for (int i = max(max(N,M),P); i < N; i++)
        for (j = max(i + 2 + 3*M, Q); j < N; j++)
           for (k = i - 2 > 0 ? i - 2 : 0); k < N; k++)
               for (l = 0; ; l++)
                   A[i][j] -= 2;
@end example
@end cartouche

Some examples of incorrect loop lower bound include:
@itemize
@item @code{for (i = 0, j = 0; ...)}: more than one initialization.
@item @code{for (i = max(a,b) + max(c,d); ...)}: not a valid conjunction.
@item @code{for (i = max(a,b) + P; ...)}: not a valid conjunction.
@item @code{for (i = a < b ? b : a; ...)}: not a (syntactically) valid ternary @code{max} operator.
@end itemize

@*

@subsection In @code{for} test statement

@*

@code{test} can be either empty (infinite loop), or of the from
@code{var opComp expressionUb <&& var opComp expressionUb2 <&& ...>
>}. That is, conjunction of upper bounds are supported via the
@code{&&} operator. The @code{expressionUb} is an affine expression,
or possibly a conjunction of expressions with the @code{min(expr1,
expr2)} operator. The @code{min} operator can either be written using
a call to the @code{min} function together with using the
@code{--polyopt-safe-math-func} flag, or with the ternary operation @code{a
< b ? a : b}. The @code{opComp} must be one of @code{<, <=, ==}.

As an illustration, all loops in the following
code form a valid SCoP.

@cartouche
@example
    for (int i = 0; i < N && i < min(min(P,Q),R); i++)
        for (j = 0; j <= (i < P ? P : i); j++)
           for (k = 0; k <= 0; k++)
               for (l = 0; ; l++)
                   A[i][j] -= 2;
@end example
@end cartouche

Some examples of incorrect loop lower bound include:
@itemize
@item @code{for (i = 0; i < 1 || i < 2)}: disjunctions are not allowed.
@item @code{for (i = 0; i < min(a,b) + min(c,d); ...)}: not a valid conjunction.
@item @code{for (i = 0; min(i, N); ...)}: missing the @code{var opComp} part.
@item @code{for (i = 0; i > P; ...)}: incorrect comparison operator.
@item @code{for (i = 0; i <  (a > b ? b : a); ...)}: not a (syntactically) valid ternary @code{max} operator.
@end itemize

@*

@subsection In @code{for} increment statement

@*
Loops must increment by step of one, and there must be a single
operation in the @code{increment} part. Typically only @code{i++},
@code{++i} and @code{i+=1} are supported increments. More complex
increments such as @code{i += one} or @code{i += 1, j += 1} are not
supported.
@*


@subsection In @code{if} conditional statement

@*

For @code{if} statements, the conditional expression can be an
arbitrary affine expression, and a conjunction of expressions with the
@code{&&} operator. @code{min} and @code{max} are allowed, provided a
@code{max} constrains the lower bound of a variable and a @code{min}
constraints the upper bound of a variable. Most standard comparison
operators are allowed: @code{<, <=, ==, >=, >}. Note that @code{else}
clause is not allowed, nor is @code{!=}.

As an illustration, all loops in the following
code form a valid SCoP.

@cartouche
@example
    if (i > max(M,N) && j == 0)
       if (k < 32 && k < min(min(i,j),P))
          A[i][j] = 42;
@end example
@end cartouche

Some examples of incorrect conditional expressions include:
@itemize
@item @code{if (i == 0 || c == 0)}: disjunctions are not allowed.
@item @code{if (i < max(A,B))}: not a valid @code{max} constraint.
@item @code{if (i == 42/5)}: not an integer term.
@end itemize


@subsection Examples

@*
We conclude by showing some examples of SCoPs automatically
detected by PolyOpt/C. Note that the only constraints for the statements
(e.g., @code{R,S,T} in the next example) involve the lack of function
calls, @emph{at most one variable is assigned in the statement}, and
using affine functions to dereference arrays.

@cartouche
@example
  alpha = 43532;
  beta = 12313;
  for (int i = 0; i < N; i++) @{
R:      v1[i] = (i+1)/N/4.0;
S:      w[i] = 0.0;
        for (j = 0; j < N; j++)
T:         A[i][j] = ((DATA_TYPE) i*j) / N;
    @}
@end example
@end cartouche


@cartouche
@example
  for (j = 1; j <= m; j++) @{
      stddev[j] = 0.0;
      for (i = 1; i <= n; i++)
      	stddev[j] += (data[i][j] - mean[j]) * (data[i][j] - mean[j]);
      stddev[j] /= float_n;
      stddev[j] = sqrt(stddev[j]);
      stddev[j] = stddev[j] <= eps ? 1.0 : stddev[j];
    @}
@end example
@end cartouche
@*




@c % /*************************************************************************
@c % /*************************************************************************

@node Optimization Paths
@chapter Optimization Paths

@*

Three main optimization paths are available in PolyOpt/C. They are
geared towards improving data locality for fewer data cache misses,
and both coarse- and fine-grain shared memory parallelization with
OpenMP.  They will be applied on all Static Control Parts that were
automatically detected in the input program. Program transformations
are generated via the @uref{http://pocc.sourceforge.net, PoCC}
polyhedral engine.

@*



@section @code{--polyopt-fixed-tiling}

@subsection Description

@*
This path automatically computes and applies a complex, SCoP-specific
sequence of loop transformations to enable parallel blocked (if possible) execution
of the SCoP. The default tile size is 32, and can be specified at
compile time only. Parallel loops are marked with OpenMP pragmas,
inner-most vectorizable loops are marked with ivdep pragmas. Parallel
or pipeline-parallel tile execution is achieved when tiling is possible.

The Pluto module is used to compute the loop transformation sequence,
in the form of a series of affine multidimensional schedules.

Giving the flag @code{--polyopt-fixed-tiling} to PolyOpt/C is
equivalent to giving the sequence:
@itemize
@item @code{--polyopt-pluto-fuse-smartfuse}
@item @code{--polyopt-pluto-tile}
@item @code{--polyopt-pluto-parallel}
@item @code{--polyopt-pluto-prevector}
@item @code{--polyopt-generate-pragmas}
@end itemize

@*

@subsection Example

@*
Given the input program:
@cartouche
@example
    for (i = 0; i < n; i++)
      for (j = 0; j < n; j++) @{
          C[i][j] *= beta;
          for (k = 0; k < n; k++)
            C[i][j] +=A[i][k] * B[k][j] * alpha;
	@}
@end example
@end cartouche

One can @emph{optionally} specify a file to set the tile sizes, to
override the default @emph{32} value. This file must be called
@code{tile.sizes}, and be stored in the current working directory. It
must contain one tile size per dimension to be tiled. For example:
@cartouche
@example
$> cat tile.sizes
16 64 1
@end example
@end cartouche

The result of @code{--polyopt-fixed-tiling} on the above example, with
the specified @code{tile.sizes} file is shown below. Note, if a
@code{tile.sizes} file exists in the current working directory it will
@emph{always} be used.
@cartouche
@smallexample
@{
    int c6;
    int c3;
    int c1;
    int c2;
    int c4;
    if (n >= 1) @{
#pragma omp parallel for private(c4, c2, c6)
      for (c1 = 0; c1 <= (((n + -1) * 16 < 0?
                   ((16 < 0?-((-(n + -1) + 16 + 1) / 16) :
                   -((-(n + -1) + 16 - 1) / 16))) : (n + -1) / 16)); c1++)
        for (c2 = 0; c2 <= (((n + -1) * 64 < 0?
                     ((64 < 0?-((-(n + -1) + 64 + 1) / 64) :
                     -((-(n + -1) + 64 - 1) / 64))) : (n + -1) / 64)); c2++)
          for (c4 = 16 * c1; c4 <= ((16 * c1 + 15 < n + -1?
                             16 * c1 + 15 : n + -1)); c4++)
#pragma ivdep
#pragma vector always
            for (c6 = 64 * c2; c6 <= ((64 * c2 + 63 < n + -1?
                               64 * c2 + 63 : n + -1)); c6++)
              (C[c4])[c6] *= beta;
#pragma omp parallel for private(c4, c2, c3, c6)
      for (c1 = 0; c1 <= (((n + -1) * 16 < 0?
                   ((16 < 0?-((-(n + -1) + 16 + 1) / 16) :
                   -((-(n + -1) + 16 - 1) / 16))) : (n + -1) / 16)); c1++)
        for (c2 = 0; c2 <= (((n + -1) * 64 < 0?
                     ((64 < 0?-((-(n + -1) + 64 + 1) / 64) :
                     -((-(n + -1) + 64 - 1) / 64))) : (n + -1) / 64)); c2++)
          for (c3 = 0; c3 <= n + -1; c3++)
            for (c4 = 16 * c1; c4 <= ((16 * c1 + 15 < n + -1?
                               16 * c1 + 15 : n + -1)); c4++)
#pragma ivdep
#pragma vector always
              for (c6 = 64 * c2; c6 <= ((64 * c2 + 63 < n + -1?
                                 64 * c2 + 63 : n + -1)); c6++)
                (C[c4])[c6] += ((((A[c4])[c3]) * ((B[c3])[c6])) * alpha);
    @}
@}
@end smallexample
@end cartouche

@*




@section @code{--polyopt-parametric-tiling}

@subsection Description

@* 

NOTE: The parametric tiling path is still experimental, and
correctness of the generated code is not guaranteed in all cases. In
particular, a known issue is when parametric tiling is applied on a
loop nest where the outer loop is sequential (wavefront creation is
required) and the inner loops are permutable but not fusable. We are
working hard to fix this remaining problem. 

To the best of our knowledge, the generated code is correct when all
statements in a (possibly imperfectly nested) loop nest can be
maximally fused. Remember that polyhedral transformations are
automatically computed before the parametric tiling pass to enforce
this property on the code when possible. The above issue impacts only
program parts where it is not possible to exhibit a polyhedral
transformation making either the outer loop parallel, or all loops
fusable in the loop nest. This is not a frequent pattern, for instance
none of the 28 benchmarks of the PolyBench 2.0 test suite exhibit this
issue.
@*

@*

This path automatically computes and applies a complex, SCoP-specific
sequence of loop transformations to enable parallel blocked execution
of the SCoP. The generated code is parametrically tiled when possible,
and the tile sizes can be specified at runtime via the
@code{__pace_tile_sizes[]} array. By default, the tile sizes are set
to 32. Parallel loops are marked with OpenMP pragmas.

The Pluto module is used to compute a loop transformation sequence
that makes tiling legal, when possible, and the PTile module performs
parametric tiling. Parallel or pipeline-parallel tile execution is
achieved if tiling is possible.

Giving the flag @code{--polyopt-parametric-tiling} to PolyOpt/C is
equivalent to giving the sequence:
@itemize
@item @code{--polyopt-pluto-fuse-smartfuse}
@item @code{--polyopt-pluto-parallel}
@item @code{--polyopt-codegen-use-ptile}
@item @code{--polyopt-codegen-insert-ptile-api}
@end itemize

@*

@subsection Example

@*
The PACE tiling API requires to use the function
@code{PACETileSizeVectorInit(int*, int, int)} to fill-in the tile
sizes. This function takes an array of integers, the number of tile
size parameters, and a unique identifier for the SCoP. This function
can be in another compilation unit, inserted automatically by the PACE
compiler, or added manually by the user. It allows to select the tile
size at run-time, before the computation starts.

The result of @code{--polyopt-parametric-tiling} on the above
@code{dgemm} example is shown below.

@cartouche
@smallexample
@{
    int ___pace_tile_sizes[3];
    PACETileSizeVectorInit(___pace_tile_sizes,3,2);
    int c2;
    int c2t1;
    int c1;
    int c3;
    int c1t1;
    float T1c3 = (float )(___pace_tile_sizes[0]);
    int c3t1;
    float T1c2 = (float )(___pace_tile_sizes[1]);
    float T1c1 = (float )(___pace_tile_sizes[2]);
    if (n >= 1) @{
      @{
        int tmpLb;
        int tmpUb;
        tmpLb = round(-1 + 1 / T1c1);
        tmpUb = round(n * (1 / T1c1) + 1 / T1c1 * -1);
#pragma omp parallel for private(c2t1, c1, c2)
        for (c1t1 = tmpLb; c1t1 <= tmpUb; ++c1t1)
          for (c2t1 = round(-1 + 1 / T1c2);
               c2t1 <= round(n * (1 / T1c2) + 1 / T1c2 * -1); ++c2t1)
            for (c1 = (c1t1 * T1c1 > 0?c1t1 * T1c1 : 0);
                 c1 <= ((c1t1 * T1c1 + (T1c1 + -1) < n + -1?
                 c1t1 * T1c1 + (T1c1 + -1) : n + -1)); c1++)
              for (c2 = (c2t1 * T1c2 > 0?c2t1 * T1c2 : 0);
                   c2 <= ((c2t1 * T1c2 + (T1c2 + -1) < n + -1?
                   c2t1 * T1c2 + (T1c2 + -1) : n + -1)); c2++)
                (C[c1])[c2] *= beta;
      @}
      @{
        int tmpLb;
        int tmpUb;
        tmpLb = round(-1 + 1 / T1c1);
        tmpUb = round(n * (1 / T1c1) + 1 / T1c1 * -1);
#pragma omp parallel for private(c2t1, c3t1, c1, c2, c3)
        for (c1t1 = tmpLb; c1t1 <= tmpUb; ++c1t1)
          for (c2t1 = round(-1 + 1 / T1c2);
               c2t1 <= round(n * (1 / T1c2) + 1 / T1c2 * -1); ++c2t1)
            for (c3t1 = round(-1 + 1 / T1c3);
                 c3t1 <= round(n * (1 / T1c3) + 1 / T1c3 * -1); ++c3t1)
              for (c1 = (c1t1 * T1c1 > 0?c1t1 * T1c1 : 0);
                   c1 <= ((c1t1 * T1c1 + (T1c1 + -1) < n + -1?
                   c1t1 * T1c1 + (T1c1 + -1) : n + -1)); c1++)
                for (c2 = (c2t1 * T1c2 > 0?c2t1 * T1c2 : 0);
                     c2 <= ((c2t1 * T1c2 + (T1c2 + -1) < n + -1?
                     c2t1 * T1c2 + (T1c2 + -1) : n + -1)); c2++)
                  for (c3 = (c3t1 * T1c3 > 0?c3t1 * T1c3 : 0);
                       c3 <= ((c3t1 * T1c3 + (T1c3 + -1) < n + -1?
                       c3t1 * T1c3 + (T1c3 + -1) : n + -1)); c3++)
                    (C[c1])[c2] += ((((A[c1])[c3]) * ((B[c3])[c2])) *alpha);
      @}
   @}
@}
@end smallexample
@end cartouche
@*

@section @code{--polyopt-parallel-only}

@subsection Description

@*
This path automatically computes and applies a complex, SCoP-specific
sequence of loop transformations to enable parallel execution of the
SCoP while improving data locality. In contrast to the other paths, no
tiling is applied on the generated program. Parallel loops are marked
with OpenMP pragmas. The Pluto module is used to compute a loop
transformation sequence that expose coarse-grain parallelism when
possible.

Giving the flag @code{--polyopt-parallel-only} to PolyOpt/C is
equivalent to giving the sequence:
@itemize
@item @code{--polyopt-pluto-fuse-smartfuse}
@item @code{--polyopt-pluto-parallel}
@item @code{--polyopt-generate-pragmas}
@end itemize
@*

@subsection Example


The result of @code{--polyopt-parallel-only} on the above @code{dgemm}
example is shown below. Note that pre-vectorization is disabled in
this mode, fixed tiling must be enabled for it to be active. To
prevent the distribution of the two statements, the user can rely on
the fine-tuning flags, e.g., @code{--polyopt-pluto-fuse-maxfuse}.

@cartouche
@smallexample
@{
    int c2;
    int c1;
    int c3;
    if (n >= 1) @{
#pragma omp parallel for private(c2)
      for (c1 = 0; c1 <= n + -1; c1++)
        for (c2 = 0; c2 <= n + -1; c2++)
          (C[c1])[c2] *= beta;
#pragma omp parallel for private(c3, c2)
      for (c1 = 0; c1 <= n + -1; c1++)
        for (c2 = 0; c2 <= n + -1; c2++)
          for (c3 = 0; c3 <= n + -1; c3++)
            (C[c1])[c2] += ((((A[c1])[c3]) * ((B[c3])[c2])) * alpha);
    @}
@}
@end smallexample
@end cartouche




@c % /*************************************************************************
@c % /*************************************************************************

@node Fine-tuning Optimizations
@chapter Fine-tuning Optimizations

@*
PolyOpt/C offer numerous tuning possibilities, use @code{--polyopt-help}
for a comprehensive list. We distinguish two main categories of
options that impact how the program will be transformed: (1) options
that control how SCoP are extracted; and (2) options that control how
each individual SCoP is transformed.
@*



@section SCoP Detection

@*

The following options are available to control how SCoP extraction is
being performed, and in particular how non-compliant features are
handled.
@itemize
@item @code{--polyopt-safe-math-func}: Consider function calls whose prototype is declared in math.h (e.g., round, sqrt, etc.) as side-effect free functions, meaning  a call to one of these functions will not break the SCoP.
@item @code{--polyopt-approximate-scop-extractor}: Over-approximate non-affine array accesses to scalars (all array cells are approximated to be read/written for each array reference).
@item @code{--polyopt-scop-extractor-verbose=1}: Verbosity option. Reports which functions have been analyzed.
@item @code{--polyopt-scop-extractor-verbose=2}: Verbosity option. Reports which SCoPs have been detected.
@item @code{--polyopt-scop-extractor-verbose=3}: Verbosity option. Reports which SCoPs have been detected.
@item @code{--polyopt-scop-extractor-verbose=4}: Verbosity option. Reports which SCoPs have been detected, print their polyhedral representation, print all nodes that broke the SCoP.
@end itemize

@*


@section Tuning Optimizations

@*
The following options are available to control PoCC, the polyhedral
engine. In particular, those control the Pluto module that is
responsible for computing the loop transformation to be applied to the
SCoP.
@itemize
@item @code{--polyopt-pocc-verbose}:
@item @code{--polyopt-pluto}: Activate the Pluto module.
@item @code{--polyopt-pluto-tile}: Activate polyhedral tiling.
@item @code{--polyopt-pluto-parallel}: Activate coarse-grain parallelization.
@item @code{--polyopt-pluto-prevector}: Activate fine-grain parallelization.
@item @code{--polyopt-pluto-fuse-<maxfuse,smartfuse,nofuse>}: Control which fusion heuristic to use (default is smartfuse).
@item @code{--polyopt-pluto-rar}: Consider Read-After-Read dependences for improved data locality.
@end itemize
@*

@c % /*************************************************************************
@c %  *************************************************************************/

@node Troubleshooting
@chapter Troubleshooting

@*
In PolyOpt/C, polyhedral programs are a constrained subset of C programs
and it can be difficult at start to understand why a program is not
detected as a SCoP. Try using the
@code{--polyopt-scop-extractor-verbose=4} option, and reading the
papers referenced below.

For any other problem, please contact directly Louis-Noel Pouchet
@uref{mailto:pouchet@@cse.ohio-state.edu,pouchet@@cse.ohio-state.edu}.

@*



@c % /*************************************************************************
@c %  *************************************************************************/

@node References
@chapter References

@c @itemize
@c @item
@table @asis

@item @anchor{baskaran.10.cgo}@b{[1]}
M. Baskaran, A. Hartono, S. Tavarageri, T. Henretty, J. Ramanujam, and
P. Sadayappan. Parameterized Tiling Revisited. In @emph{International
Symposium on Code Generation and Optimization (CGO'10)}, Apr 2010.

@item @anchor{bastoul.04.pact}@b{[2]}
C@'edric Bastoul. Code Generation in the Polyhedral Model Is Easier Than
You Think. In @emph{IEEE International Conference on Parallel
Architecture and Compilation Techniques (PACT'04)}, Sept 2004.

@item @anchor{bondhugula.08.pldi}@b{[3]}
Uday Bondhugula and Albert Hartono and J. Ramanujam and P. Sadayappan. A
Practical Automatic Polyhedral Program Optimization System. In @emph{ACM
SIGPLAN Conference on Programming Language Design and Implementation
(PLDI'08)}, Jun 2008.

@item @anchor{feautrier.91.ijpp}@b{[4]}
Paul Feautrier. Dataflow Analysis of Array and Scalar References. In @emph{Intl. Journal of Parallel Programming}, 20(1):23--53, 1991.

@item @anchor{feautrier.92-b.ijpp}@b{[5]}
Paul Feautrier. Some efficient solutions to the affine scheduling problem.
 Part II, Multidimensional time. In @emph{Intl. Journal of Parallel Programming}, 21(5):389--420, 1992.

@item @anchor{pouchet.11.popl}@b{[6]}
Louis-Noel Pouchet, Uday Bondhugula, Cédric Bastoul, Albert Cohen,
J. Ramanujam, P. Sadayappan and Nicolas Vasilache. Loop
Transformations: Convexity, Pruning and Optimization. In @emph{ACM
SIGACT-SIGPLAN Symposium on Principles of Programming Languages
(POPL'11)}, Jan 2011.


@end table
@c @end itemize




@c % /*************************************************************************
@c %  *                       PART VI: END OF THE DOCUMENT                    *
@c %  *************************************************************************/
@c @unnumbered Index

@c @printindex cp

@bye




